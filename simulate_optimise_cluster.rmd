## Simulate, optimise, partition: algorithmic diagrams of the pluri-dimensional field from 1953 onwards

## todo

- get Halpern
- add in Boyd
- bowker on cybernetic strategy


## Introduction


Contemporary attempts to find patterns in data, ranging from the now mundane technologies of touchscreen gesture recognition through to mammoth infrastructure-heavy practices of deep learning conducted by major business, scientific and government actors to find cats [@Markoff_2012], the Higgs boson, credit card fraud or terrorists, rely on  group of algorithms intensively developed during the 1950-60s in physics, engineering and psychology.   Whether we designate them as pattern recognition, data mining or machine learning (all terms that first came into play during the 1950s), the standard account enunciated by proponents (and opponents) of these techniques is that they uncover patterns in data that cannot appear directly to the human eye, either because there are too many items for anyone to look at, or because the patterns are too subtly woven through in the data. 

In the standard contemporary narratives of their efficacy and indeed necessity,  the spectrum of differences accommodated under the rubric of pattern is striking. Pattern here is understood to encompass language, images, measurements and traces of many different kinds. These techniques -- although that term is problematic because it suggests skilled hands doing something; I will refer to them as _operations_ -- diagram a new kind of continuum or field which accommodates seemingly very different things -- terrorists, fundamental particles, photographs, market transactions, utterances and gestures -- more or less uniformly.
        
It may be that pattern across all of these differences has a distinctive pattern. What counts as pattern, I will suggest, can only be understood by taking into account the transformations in seeing, finding, comparing and categorising associated with algorithmic pattern recognition. From the plethora of operations in current use, three diagrams developed in the Cold War era  operate in contemporary modes of pattern finding: Monte Carlo simulation as a way of generating flows of random numbers to explore their effect on each other; gradient descent as a way of finding maximum or minimum value numerical solutions -- convex optimisations -- to systems of equations that cannot be solved analytically; and finally classifier algorithms that partition groups or clusters in data. The operations expressed in these diagrams took shape in different places -- nuclear physics, control systems engineering and psychology, but soon moved across boundaries between academic disciplines, and between  universities, industry, the military, and government.  Each of these operations configures a different mode of moving through data. These different modes of mapping and partitioning flow through into contemporary scientific, technological, business and governmental problematizations. The different perspectives on event, trajectory, and proximity they embody imbue many power relations, forms of value and the play of truth/falsehood today. In each case, the operation contributed something to the operational field that today that no longer exists purely or in isolation, but in combination with each other.


## What are continuities in the operational field?

> If the operatives of the Cold War could reserve for themselves the position of gray eminence, the distant advisor to the executive power, the new spaces of collectively intelligent networks and the asymmetrical relations these put in place demand instead the more difficult position of gray immanence [@Fuller_2012, 32].

The shift between grey eminence or powerful advisors to executive power and grey immanence in intelligent networks is precisely the movement that we might delineate by paying attention to the operations of simulation, optimisation and partitioning that underpin and in many ways epitomise the tain of social network media, contemporary health and biomedical knowledges and credit ratings, to name a few. By looking at certain operations for working with data, number, probability and pattern that took shape deep in the epistemic cultures of the Cold War, we can begin to see the emergence of an expansive field, a field that is neither that of the lifeworld (lived, urban, etc.) linked to a subject position (the grey eminences of Cold War science, as described for instance in _How Reason Almost Lost Its Mind_ [@Erickson_2013]  nor a mathematical abstraction (Euclidean space), nor even a single assemblage, but something more like an _operational field_ in Michel Foucault's sense of the term [@Foucault_1972, 106].[^1.1] The dimensionality and scale of this field undergoes constant expansion and accumulation, partitioning and aggregation. Through it, in it, near it, derived from it, many different artifices, devices, arrangements, operations and processes accumulate. It is an power-laden operational space that traverses and structures many aspects of our lives, but is only intermittently sensible or palpable to us. It appears at the intersection of many scientific disciplines, various infrastructures, operations, and institutions. It is object and domain of much work and investment in management, enterprise and State. 

[^1.1]: Foucault writes:

    > I now realize that I could not define the statement as a unit of a linguistic type (superior to the phenomenon of the word, inferior to the text); but that I was dealing with an enunciative function that involved various units (these may sometimes be sentences, sometimes propositions; but they are sometimes made up of fragments of sentences , series or tables of signs, a set of propositions or equivalent formulations); and, instead of giving a 'meaning' to these units, this function relates them to a field of objects; instead of providing them with a subject, it opens up for them a number of possible subjective positions; instead of fixing their limits, it places them in a domain of coordination and coexistence; instead of determining their identity, it places them in a space in which they are used and repeated. In short, what has been discovered is not the atomic statement - with its apparent meaning, its origin, its limits, and its individuality - but the operational field of the enunciative function and the conditions according to which it reveals various units (which may be, but need not be, of a grammatical or logical order) [@Foucault_1972,  106]

The composition of this field is distributed across methods, operations, infrastructures, forms of expertise, models. It loosely coalesces along some diagrams of pluri-dimensionality that the current 'examples' highlight.  We can't understand the force of the contemporary operational field without moving backwards and seeing how those power-laden operations proliferated and coalesced. Authoritative and recent accounts of Cold War rationality have strongly dramatised the part cybernetics play in re-structuring human-machine-organism relations in various contexts ranging across engineering, psychology, management, military strategy and anthropology [@Bowker_1993; @Edwards_1996; @Halpern_2015]. The general point in much of this work is that the ostensible focus of cybernetic systems on control, automation and cognition was quickly generalized into more much diffuse accounts and propositions concerning organisations, states, populations or individual experience. The diagrammatic operations I describe here are also intimately linked with transformations in what counts as pattern, recognition, learning and intelligence, but their mode of semiosis somewhat differs from cybernetics. It is an operational generalization rather than an enunciative one, and takes place, unsurprisingly given its contemporary materialisation, in code. 

## Exact means simulated, simulated means open




```{r pi_monte_carlo, include=FALSE, echo = TRUE, message = FALSE, warning = FALSE, comment= NA, fig.cap= '', dev='pdf'}
    N <- 100000
    R <- 1
    x <- runif(N, min= -1, max= R)
    y <- runif(N, min= -1, max= R)
    is.inside <- (x^2 + y^2) <= R^2
    pi.estimate <- 4 * sum(is.inside) / N
    pi.estimate
    plot.new()
    plot.window(xlim = 1.1 * R * c(-1, 1), ylim = 1.1 * R * c(-1, 1))
    points(x[ is.inside], y[ is.inside], pch = '.', col = "blue")
    points(x[!is.inside], y[!is.inside], pch = '.', col = "red")
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/pi_monte_carlo-1.pdf}
        \caption{A Monte Carlo simulation of $\pi$}
  \label{fig:pi_monte_carlo}
\end{figure}

In 1953, Nicholas Metropolis, the Artur and TBA Rosenbluths, Edward and TBA Teller, all physicists working at Los Alamos, were considering ‘the properties of any substance which may be considered as composed of interacting individual molecules’ [@Metropolis_1953, 1087]. These properties might be, for instance, the flux of neutrons in a hydrogen bomb detonation. In their short, evocatively titled and still widely cited paper 'Equations of state for fast calculating machines' (over 20,000 citations according to Google Scholar; over 14,000 according to Thomson Reuters Web of Knowledge), they describe how they used computer simulation to manage with the inordinate number of possible interactions in a substance, and to thereby come up with a statistical description of the properties of the substance. While statistical descriptions of the properties of things are not new, [^1.2]  their model system consists of a square containing only a few hundred particles. (This space is a typical multivariate joint distribution.) These particles are at various distances from each other and exert forces (electric, magnetic, etc.) on each other dependent on the distance. In order to estimate the probability that the substance will be in any particular state (fissioning, vibrating, crystallising, cooling down,  etc.), they needed to integrate over the many dimensional space comprising all the distance and forces between the particles. The dimensions of the space in which all of the variables describing the velocity, momentum, rotation, mass for each of the several hundred particles is already an expansive one.  As they write, ‘it is evidently impossible to carry out a several hundred dimensional integral by the usual numerical methods, so we resort to the Monte Carlo method’ (1088), a method that Nicholas Metropolis and Stanislaw Ulam had several years previously already described in an earlier paper [@Metropolis_1949]. Here the problem is that the turbulent randomness of events in a square containing a few hundred particles thwarts calculations of the physical properties of the substance. They substitute for that non-integrable turbulent randomness a controlled flow of random variables generated by a computer. While still somewhat random (i.e. pseudo-random), these Monte Carlo variables taken together approximate to the integral of the many dimensional space. 

A toy example to show the intuition of Monte Carlo simulation is shown in Figure \ref{fig:pi_monte_carlo}. The point of this simulation, which comprises a half dozen lines of code, is to calculate the value of the mathematical constant $\pi$, a value that describes the ratio between the radius and circumference of a circle. In this Monte Carlo simulation of $\pi$, 100000 points are randomly generated, each point described by an x-y coordinate. Given the formula for the area of a circle ($\pi r^2$) and assuming the radius of the circle is 1 unit, the algorithm tests each random point to see if falls inside the circle. The ratio for $\pi$ is given by dividing the number of points inside the circle by the total number of randomly generated points and then multiplying by 4 ( since if the radius of the circle =1, then the diameter =2, and therefore, the total area of the bounding box = $2 \times 2$, so $\pi = 4 \times p$  the proportion inside the circle). The point of this demonstration is not to re-state the value of $\pi$, but to suggest that we can see here an inexact, probabilistic calculation of a number that epitomises mathematical precision and geometric ideal form (the circle). Monte Carlo simulation we might say puts ideal form  on a computational footing. 

```{r gibbs_normal_bivar, include=FALSE, echo=FALSE,  fig.cap= '', cache=TRUE, message=FALSE, warning=FALSE, comment=NA, dev='pdf'} 
  
    Niter=6*10^4
    v=1
    da = sample(c(rnorm(10^2), 2.5 + rnorm(4 * 10^2)))

    like = function(mu) {
        sum(log((0.2 * dnorm(da - mu[1]) + 0.8 * dnorm(da - mu[2]))))
    }

    mu1 = mu2 = seq(-2, 5, le = 250)
    lli = matrix(0, ncol = 250, nrow = 250)
    for (i in 1:250) 
      for (j in 1:250) 
        lli[i, j] = like(c(mu1[i],   mu2[j]))

    x = prop = runif(2, -2, 5)
    the = matrix(x, ncol = 2)
    curlike = hval = like(x)
    for (i in 2:Niter) {
        pp = 1/(1 + ((0.8 * dnorm(da, mean = the[i - 1, 2]))/(0.2 * dnorm(da, mean = the[i - 1, 1]))))
        z = 2 - (runif(length(da)) < pp)
        prop[1] = (v * sum(da[z == 1]))/(sum(z == 1) * v + 1) + 
            rnorm(1) * sqrt(v/(1 + sum(z == 1) * v))
        prop[2] = (v * sum(da[z == 2]))/(sum(z == 2) * v + 1) + 
            rnorm(1) * sqrt(v/(1 + sum(z == 2) * v))
        curlike = like(prop)
        hval = c(hval, curlike)
        the = rbind(the, prop)
    }

    image(mu1, mu2, -lli, col = topo.colors( 55 ), xlab = expression(mu[1]), ylab = expression(mu[2]))
    contour(mu1, mu2, -lli,drawlabels=FALSE,  nle = 50, add = T)
    points(the[1:100, 1], the[1:100, 2], cex = 0.2, pch = 17)
    min=0
    max= 50000
    points(the[min:max, 1], the[min:max, 2], cex = 0.8, pch=4)
    si = sample.int(dim(the)[1], 100)
    lines(the[si, 1], the[si, 2], cex = 0.6, pch = 19)
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/gibbs_normal_bivar-1.pdf}
        \caption{A Markov Chain Monte Carlo simulation of two normal distributions}
  \label{fig:gibbs_normal_bivar}
\end{figure}

The contour plot in Figure \ref{fig:gibbs_normal_bivar} was generated by a variant of Monte Carlo simulation called MCMC -- Markov Chain Monte Carlo simulation -- that has greatly transformed much statistical practices since the early 1990s (see [@Mcgrayne_2011] for a popular account). Like the simulation of $\pi$, this simulation calculates particular numbers, this time $\mu_1$ and $\mu_2$, the mean values of two probability distributions. This seemingly very simple simulation of the contours of two normally-distributed sets of numbers shows four main diagrammatic operations. First, pluri-dimensional fields arise at the intersection of different axes or vectors of variation. While the graphic plot here is two dimensional in this case, in can be algorithmically generalized to much higher dimensions, combining many more variables. Second, although Descartes may have first formalised the coordinate geometry using axes in the form of the Cartesian plane, we can see in Figure \ref{fig:gibbs_normal_bivar}  that this plane has a different consistency or texture. While the Cartesian axes are intact with their scales and marked intervals, the field itself is populated by a plural flux of random numbers   in the Monte Carlo simulation. The topographic convention of showing heights using contour lines work in Figure \ref{fig:gibbs_normal_bivar} to render visible continuously varying distributions of values across multi-dimensions. The curves of these contours derive  from large populations of random numbers generated in the simulation. While the curves of the contour lines join elevation that have the same value, the continuous undulation of values cannot be captured by any line or regular geometrical form.  Third, superimposed on the contour lines, a series of steps or a path explore that irregular topography of the pluri-dimensional field. This exploration appears as in the dense mass of black points on the figure deriving from the further flows of random numbers moving towards the highest point, the peak of the field, guided by continuous testing of convergence.  Finally, plot as a whole graphs  the different values of the means ($\mu_1, \mu_2$) of the variables distributed over a range of different possible values.  We need know nothing about what such variables relate to, apart from the fact they are something to do with probabilities, with assigning numbers to events  in some setting, whether they be the possible collisions of neutrons or the likelihood of an asthma attack. A set of connected points starting on the side of the one of the peaks and clustering on the peak shows how the MCMC algorithm explores the contours. Peaks -- zones that attract events or beliefs -- are sometimes difficult to find in complicated terrain. MCMC is a way of finding peaks.  

Compare this to a contemporary epidemiological model used by the Public Health Authority in the aftermath of the 2009 swine flu epidemic.  Here the peaks are also important, but now as a way of trying to sort out what in the course of an epidemic is due to force of infection of a virus and what is due to the social practices of people influenced by the widely publicised awareness of the onset of an epidemic. Here logistic supply chains of numbers deriving from doctors, pathology labs and population surveys are brought together to simulate in real time the trajectory of the pandemic. 

[^1.2]: Isabelle Stengers provides an excellent account of some of the nineteenth century development of thermodynamics [@Stengers_2011]. The history of statistics since the late seventeenth century obviously forms part of the background here [[@Stigler_1986; @Hacking_1975]. 


## Optimise in order to learn: 1957

We know from the histories of Cold War rationality that cognition and calculation are tightly entwined. Cold War cognition calculates its chances of winning, error, loss, costs, times and delays. But the mechanisms of this entwining of cognition and calculation are not necessarily obvious. Because of the prevalence of discursive analyses of power and knowledge, the subtle mechanisms and diagrammatic operations of cognitive calculation sometimes remain opaque. But it is precisely these mechanisms that flow along long fault-lines into contemporary knowledge apparatuses with all their power-generating armatures in areas such as security or online media. The operations of these mechanisms are often quite localised and in some cases trivial (e.g. fitting a straight line to some points), but their operations accumulate and generalise in ways that sometimes produce strategic, even hegemonic effects in contemporary culture. While there are quite a few operations that might be examined from this perspective, the case of the Perceptron from 1957 is evocative because of both its departure from cybernetic inspirations and its subsequent re-enactments during the 1980-1990s in the form of neural nets, in the very abundant support vector machines of the 1990s [@Cortes_1995], and today in the massively ramified form of 'deep learning' [@Hinton_2006]. 

'Learning' is pivotal to diagrammatic operation of machines such as the perceptron. While initially framed in terms of rational actors playing games (for instance in 1944 in Oskar Morgenstern and John von Neumann's _Theory of Games and Behaviour_ [@VonNeumann_2007]; see [@Erickson_2013, 133-134]), the locus of learning shifts diagonally through diagrams, plans and equations into different arrangements that had much less to do with the agential dilemmas of  Cold War military strategy.  While the perceptron retains some figurative aspects of its biological inspiration in the neurone, these figurative aspects are rapidly overlaid and displaced by asignifying processes that have continued to mutate in subsequent decades. The so-called 'learning problem' and the subsequent theory of learning machines was developed largely by researchers in the 1960-1980s, but based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@Rosenblatt_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network on a computer at the Cornell University Aeronautical Laboratory in 1957. A psychologist working in an aeronautical laboratory sounds rather odd, but given that the title of Rosenblatt's 1958 publication  -- 'The perceptron: A probabilistic model for information storage and organization in the brain' -- already suggested an intersection between statistics (probabilistic models), computation (information storage and organization), and neuroscience ('brain'), perhaps Rosenblatt's  cross-campus mobility is symptomatic of the diagonal movement occurring around learning. The very term 'perceptron' already amalgamates the organic and the organic, the psychological and the technological in a composite form. 

Like the Monte Carlo simulations, the perceptron operates according to a very simple intuition: a machine can learn by classifying things according to their position in a pluri-dimensional data space. 'Geometrically speaking,' writes Vladmir Vapnik (a machine learning researcher famous for his work on the support vector machine),  'the perceptron divides the space $X$ into two parts separated by a piecewise linear surface. ... Learning in this model means finding appropriate coefficients for all neurons using given training data' [@Vapnik_1999, 3]. If the Monte Carlo simulation generated a space in which many different variables could be integrated in exploring probability distributions, devices such as the perceptron configure a space in which classification takes place on a moving substrate of relations between data points. 

```{r perceptron, fig.cap='', include=FALSE, dev='pdf'}
    source('perceptron.r')
    eta  <- .01
    cols <- rainbow(10) # colors for visualization
    w    <- c(0,1,-1)
    m    <- matrix(c(2,4, 1,.5, .5,1.5, 0,.5), nrow=2)
    y    <- c(1,1,-1,-1)


    plotHeader(m, eta)
    trainPerceptron(m, y, w, eta)


```
\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/perceptron-1.pdf}
        \caption{Perceptron learns to separate}
  \label{fig:perceptron}
\end{figure}


Secondly, the role of learning. As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@Vapnik_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@Minsky_1969], later work showed that perceptrons could 'learn universally.' For present purposes, the key point is not that neural networks have turned out by the 1980s to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (for instance, in commercial applications such as drug prediction [@Dahl_2013]).  Rather, the important point is that it began to introduce learning machines as an ongoing project in which trying to understand what machines can learn, and to predict how they will classify or predict became central concerns precisely because machines didn't seem to classify or predict things at all well. At the same time, and less visibly,  implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines became more statistically sophisticated. 

The practice of learning here owes much more to logistics than it does to artificial intelligence in the classical sense. That is, learning occurs through and takes the form of optimisation. Optimisation in turn is understood in in terms mathematical functions located in high-dimensional spaces that cannot be analysed in closed-forms, but only explored looking for maxima or minima. Optimisation algorithms such as as gradient descent or expectation maximisation (EM) are the key components here. The difference between 
That is, the theory of machine learning alongside decision theory was interwoven with a set of concepts, techniques and language drawn from statistics. Just as humans, crops, habitats, particles and economies had been previously, learning machines became entwined with statistical methods. Not only in their reliance on the linear model as a common starting point, but in theories of machine learning, statistical terms such as bias, error, likelihood and indeed an increasingly thorough-going probabilistic framing of learning machines emerged. 

Learning machines optimise rather than cognize. The plot of a few points in a two dimensional space shown in Figure \ref{fig:perceptron} again has to stand in for a much more voluminous, densely populated and pluri-dimensional space. The different shapes of the points index different categories of things (for example, `male` vs. `female`). The lines in this figure are the work of a perceptron learning to classifying the points by searching for lines that divide the space. Starting with an arbitrary line, the perceptron tests whether a line effectively separates the different categories. If it does not cleanly separate them, the algorithm incrementally adjusts the parameters that define the slope and intercept until the line does run cleanly between the different categories. It easily may be that the different categories overlap, in which case, the perceptron algorithm will never converge since it cannot find any line that separates them.

For our purposes the point is that any learning that occurs here lies quite a long way away from biological figure of the neurone. Some biological language remains -- 'activation functions', for instance, figure in the code that produces the lines in  -- but the algorithmic process of testing lines and adjusting weights in order to find a line or plane or hyperplane (in higher dimensional data) very definitely an optimisation process in which errors are gradually reduced to a minimum. Furthermore, the diagrammatic operation of the perceptron  -- repeating drawing of lines in order to classify -- appears in a number of variations in the following decades. Some of these variations -- linear discriminant analysis, logistic regression, support vector machine -- generalise the process of finding separating lines or planes in data in quite complicated ways in order to find more supple or flexible classifications. The perceptron is not unique in doing this. Subsequent developments, including various machine learning models such as logistic regression, neural nets, support vector machines, _k_ nearest neighbours and others present variations on the same diagrammatic operation: pattern recognition entails learning to classify; learning to classify means finding a way of best separating or partitioning  a pluri-dimensional data space; the best partition optimises by reducing the number of misclassification errors. It is not surprising that later iterations and variations of the perceptron proliferated the process of optimisation. For instance, neural nets, current favourite machine learning techniques for working with large archives of image and sound, aggregate many perceptrons in layered networks, so that the perceptron's diagrammatic operation of classification can be generalised to highly complex patterns and shapes in the field of data. No matter how complex the classifiers become, they still  propagate the same diagrammatic operation of drawing a line and testing its power to separate. 


## Ramified movements

Cold War information theory says information is that which allows a move down a decision tree [@Solovey_2012, 103]. The final diagrammatic operation concerns decisions. Decisions -- as the term itself suggests -- are a kind of cut. But decisions often have a complicated branching structure, at least, in the procedural rationality typical of Cold War operations research and structure. In procedural rationality, branches in a decision tree stem from rules that embody the expert knowledge of the grey eminences of management science and its cognate power-knowledge formations.  Increasingly, these rules are shaped by optimisation and modelling procedures  that seek to allocate resources most efficiently (especially in the field of operations research; see [@Erickson_2013, 79]). 

The decision tree was and remains a key epistemic construct in Cold War closed-world thinking in its attempt to represent optimal allocation of resources. The development of procedural rationality based on various algorithmic procedures (linear programming, dynamic programming [@Bellman_1961]) was however paralleled in pattern recognition and machine learning by a different form of decision tree. During these same decades the decision tree itself is reorganised in data mining and pattern recognition into a rather different kind of device that in some ways owes more to the systems of classification associated with taxonomy or natural history. The decision tree is no longer a way of regulating information flow towards optimum resource allocation (missiles, cargoes, troops, etc.). In classification and regression trees, branches are  instead something to be learned from the data rather than from experts or their optimization techniques. Decision rules are replaced by learning algorithms that partition according to quasi-statistical measures of mixedness or purity. To give a brief indication of how the decision tree has changed its mode of operation, we might think of the Microsoft Kinect game controller, a popular gaming interface that uses video cameras and decision tree algorithm to learn to classify players' gestures and movements, and thereby allow them to play a computer game without touching buttons, levels or holding a game controller. The decision algorithm, coupled with the imaging system, translates the mobility and fluidity of gesture into a set of highly coded movements in the on-screen space of the game. 

In a certain sense, the decision tree (and its contemporary incarnation in the very popular random forest [@Breiman_2001]) dismantles the classical tree with its reference to kinds of being. It also obviates in certain ways the decision tree of procedural rationality as a distillation of expert knowledge. And finally, it potentially transforms the biopolitical rendering of differences through specific attributes of individual and populations into a rather mobile matrix of potential mixtures and overlaps. Take the case of the `iris` dataset, one of the most famous in the machine learning scientific literature. The eugenicist statistician Ronald A. Fisher first used this dataset in his work on the important linear discriminant analysis technique in the late 1930s [@Fisher_1938]. The `iris` in some ways innocuously enough epitomises the modelling of species differences via measurements of biological properties (in this, measurements of such things as petal widths and lengths of irises growing in the Gaspé Peninsula in Canada). 

```{r decision_tree, echo=FALSE, include=FALSE, dev='pdf'}
library(ggplot2)
library(tree)
data(iris)
tree1 <- tree(Species ~ Sepal.Width + Petal.Width,  data = iris)
par(mfrow = c(1,2))
plot(tree1)
text(tree1)
plot(iris$Petal.Width,iris$Sepal.Width, cex=0.65,  col=as.numeric(iris$Species), pch = as.numeric(iris$Species))
partition.tree(tree1,label="Species",add=TRUE)
legend(1.75,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
```

\begin{figure}
  \centering
      \includegraphics[width=0.99\textwidth]{figure/decision_tree-1.pdf}
        \caption{Decision tree model on `iris` data}
  \label{fig:iris_tree}
\end{figure}


The plot on the left in Figure \ref{fig:iris_tree} shows the decision tree and the plot on the right shows the three _iris_ species _virginica_,  _setosa_ and _versicolor_  plotted by petal and sepal widths.  [HERE] As the plot on the right shows, most of the measurements are well clustered. Only the _setosa_ petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. This means that the decision tree shown on the left has little trouble classifying the irises. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is _setosa_. Hastie, Tibshirani and Friedman  suggest that 'a key advantage of the recursive binary tree is its interpretability. The feature space partition is fully described a by single tree.  ... This representation is popular among medical scientists, perhaps because it mimics the way a doctor thinks.  The tree stratifies the population into strata of high and low outcome, on the basis of patient characteristics' [@Hastie_2009, 306-7]. I would differ from them on this point.  Decision trees do indeed have a rich  medical, as well  commercial and industrial history of use. And yes, decision trees and their later variations (such  as`C4.5`, the 'top' data-mining algorithm according to a survey of data miners in 2009 [@Wu_2008] was developing during the 1980s in response to Friedman's work on decision trees) are often presented as easy to use because they are 'not unlike the serious of troubleshooting questions you might find in your car's manual to help determine what could be wrong with the vehicle' [@Wu_2008,2]. (But who actually reads car manuals?) While that scenario is unlikely today, especially as Google sends autonomous-driving cars out onto the roads of California undoubtedly controlled by a variety of classifiers such as decision trees, neural networks and support vector machines, the recursive partitioning technique still has a great deal of traction in machine learning practice precisely because of its simplicity. 


The problem with _iris_ is that the species actually ontically separate. The pattern of separation that the decision tree algorithm finds exists in the world for us too, except when, as happens, species of iris hybridise with each other. In many cases, things as Whitehead suggests, are not cleanly separable. Often there is some pattern of separation, perhaps in the form of overlapping clusters or clouds of points, but not enough to define a simple set of decision rules.  (As we will see that support vector machines, with their 'maximum margin hyperplanes,' take these overlaps as given, and build models focused on the overlaps.) How then does a decision tree decide how to split things? What counts as a good split has been a long standing topic of debate in the decision tree literature. Choosing where to cut: this is a key problem for classification or decision trees. As Malley, Malley and Pajevic observe, 'the challenge is to define _good_ when its clear that no obviously excellent split is easily available' [@Malley_2011, 121].  The definition of 'good' that has emerged in the decision tree literature is centred on 'node purity.'


We are dealing here with situations that differ quite radically from the either the artificial intelligence of the Cold War, or the expert systems that followed them in the 1970s and 1980s. Both still assumed that there could be in principle separation or discrmination. But 

1979: Decisions, Ensembles and clusters: the lone expert replaced by statistical algorithm that learns from the data (decision trees - 1980s); the algorithm replaced by ensembles of algorithms (2000) that collectively learn.  Cluster has this double sense of finding groups (the verb to cluster) and a collection (the cluster). Cluster as infrastructural principle (Google Compute 2011)

## Conclusion



## References


