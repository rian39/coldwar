## Simulate, optimise, partition: algorithmic diagrams of pattern from 1953 onwards


## Introduction


Contemporary attempts to find patterns in data, ranging from the now mundane technologies of touchscreen gesture recognition through to mammoth infrastructure-heavy practices of deep learning conducted by major business, scientific and government actors to find cats [@Markoff_2012], the Higgs boson, credit card fraud or terrorists, rely on  group of algorithms intensively developed during the 1950-60s in physics, engineering and psychology.   Whether we designate them as pattern recognition, data mining or machine learning (all terms that first came into play during the 1950s), the standard account enunciated by proponents (and opponents) of these techniques is that they uncover patterns in data that cannot appear directly to the human eye, either because there are too many items for anyone to look at, or because the patterns are too subtly woven through in the data. 

In the standard contemporary narratives of their efficacy and indeed necessity,  the spectrum of differences accommodated under the rubric of pattern is striking. Pattern here is understood to encompass language, images, measurements and traces of many different kinds. These techniques -- although that term is problematic because it suggests skilled hands doing something; I will refer to them as _operations_ -- diagram a new kind of continuum or field which accommodates seemingly very different things -- terrorists, fundamental particles, photographs, market transactions, utterances and gestures -- more or less uniformly.
        
What counts as pattern today, I will suggest, can only be understood by taking into account the transformations in seeing, finding, counting, comparing and categorising associated with algorithmic pattern recognition taking shape in the mid-twentieth century. Despite their often somewhat a-historical invocations, the patterns recognised in pattern recognition have an historically concrete specificity. From the plethora of operations in current use, three diagrams developed in the Cold War era  operate in contemporary modes of pattern finding: Monte Carlo simulation as a way of generating flows of random numbers to explore irregular probability distributions; gradient descent as a way of doing _convex optimisations_ -- maximum or minimum value numerical solutions to systems of equations that cannot be solved analytically; and finally recursive partitioning algorithms that classify groups or identify clusters in data. The operations expressed in these diagrams took shape in different places -- nuclear physics, control systems engineering and psychology, but soon moved across boundaries between academic disciplines, and between  universities, industry, the military, and government.  Each of these operations configures a different mode of moving through data in order to produce patterns. These different modes of mapping and partitioning flow through into contemporary scientific, technological, business and governmental problematizations. The different perspectives on event, trajectory, and proximity they embody imbue many power relations, forms of value and the play of truth/falsehood today. In each case, the operation contributed something to the operational field that today that no longer exists purely or in isolation, but in combination with each other.


## What are continuities in the operational field?

> If the operatives of the Cold War could reserve for themselves the position of gray eminence, the distant advisor to the executive power, the new spaces of collectively intelligent networks and the asymmetrical relations these put in place demand instead the more difficult position of gray immanence [@Fuller_2012, 32].

The shift between grey eminence or powerful advisors to executive power and grey immanence in intelligent networks is precisely the movement that we might delineate by paying attention to the operations of simulation, optimisation and partitioning that underpin and in many ways epitomise the tain of social network media, contemporary health and biomedical knowledges and credit ratings, to name a few. By looking at certain operations for working with data, number, probability and pattern that took shape deep in the epistemic cultures of the Cold War, we can begin to see the emergence of an expansive field, a field that is neither that of the lifeworld (lived, urban, etc.) linked to a subject position (the grey eminences of Cold War science, as described for instance in _How Reason Almost Lost Its Mind_ [@Erickson_2013]  nor a mathematical abstraction (Euclidean space), nor even a single assemblage, but something more like an _operational field_ in Michel Foucault's sense of the term [@Foucault_1972, 106].[^1.1] The dimensionality and scale of this field undergoes constant expansion and accumulation, partitioning and aggregation. Through it, in it, near it, derived from it, many different artifices, devices, arrangements, operations and processes accumulate. It is an power-laden operational space that traverses and structures many aspects of our lives, but is only intermittently sensible or palpable to us. It appears at the intersection of many scientific disciplines, various infrastructures, operations, and institutions. It is object and domain of much work and investment in management, enterprise and State. 

[^1.1]: Foucault writes:

    > I now realize that I could not define the statement as a unit of a linguistic type (superior to the phenomenon of the word, inferior to the text); but that I was dealing with an enunciative function that involved various units (these may sometimes be sentences, sometimes propositions; but they are sometimes made up of fragments of sentences , series or tables of signs, a set of propositions or equivalent formulations); and, instead of giving a 'meaning' to these units, this function relates them to a field of objects; instead of providing them with a subject, it opens up for them a number of possible subjective positions; instead of fixing their limits, it places them in a domain of coordination and coexistence; instead of determining their identity, it places them in a space in which they are used and repeated. In short, what has been discovered is not the atomic statement - with its apparent meaning, its origin, its limits, and its individuality - but the operational field of the enunciative function and the conditions according to which it reveals various units (which may be, but need not be, of a grammatical or logical order) [@Foucault_1972,  106]

The composition of this field is distributed across methods, operations, infrastructures, forms of expertise, models. It loosely coalesces along some diagrams of pluri-dimensionality that the current 'examples' highlight.  We can't understand the force of the contemporary operational field without moving backwards and seeing how those power-laden operations proliferated and coalesced. Authoritative and recent accounts of Cold War rationality have strongly dramatised the part cybernetics play in re-structuring human-machine-organism relations in various contexts ranging across engineering, psychology, management, military strategy and anthropology [@Bowker_1993; @Edwards_1996; @Halpern_2015]. The general point in much of this work is that the ostensible focus of cybernetic systems on control, automation and cognition was quickly generalized into more much diffuse accounts and propositions concerning organisations, states, populations or individual experience. The diagrammatic operations I describe here are also intimately linked with transformations in what counts as pattern, recognition, learning and intelligence, but their mode of semiosis somewhat differs from cybernetics. It is an operational generalization rather than an enunciative one, and takes place, unsurprisingly given its contemporary materialisation, in code. 

## Exact means simulated, simulated means open

```{r pi_monte_carlo, include=FALSE, echo = TRUE, message = FALSE, warning = FALSE, comment= NA, fig.cap= '', dev='pdf'}
    N <- 100000
    R <- 1
    x <- runif(N, min= -1, max= R)
    y <- runif(N, min= -1, max= R)
    is.inside <- (x^2 + y^2) <= R^2
    pi.estimate <- 4 * sum(is.inside) / N
    pi.estimate
    plot.new()
    plot.window(xlim = 1.1 * R * c(-1, 1), ylim = 1.1 * R * c(-1, 1))
    points(x[ is.inside], y[ is.inside], pch = '.', col = "blue")
    points(x[!is.inside], y[!is.inside], pch = '.', col = "red")
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/pi_monte_carlo-1.pdf}
        \caption{A Monte Carlo simulation of $\pi$}
  \label{fig:pi_monte_carlo}
\end{figure}

In 1953, Nicholas Metropolis, the  Rosenbluths, and the Tellers, all physicists working at Los Alamos, were considering ‘the properties of any substance which may be considered as composed of interacting individual molecules’ [@Metropolis_1953, 1087]. These properties might be, for instance, the flux of neutrons in a hydrogen bomb detonation. In their short, evocatively titled and still widely cited paper 'Equations of state for fast calculating machines' (over 20,000 citations according to Google Scholar; over 14,000 according to Thomson Reuters Web of Knowledge), they describe how they used computer simulation to manage with the inordinate number of possible interactions in a substance, and to thereby come up with a statistical description of the properties of the substance. While statistical descriptions of the properties of things are not new, [^1.2]  their model system consists of a square containing only a few hundred particles. (This space is a typical multivariate joint distribution.) These particles are at various distances from each other and exert forces (electric, magnetic, etc.) on each other dependent on the distance. In order to estimate the probability that the substance will be in any particular state (fissioning, vibrating, crystallising, cooling down,  etc.), they needed to integrate over the many dimensional space comprising all the distance and forces between the particles. The dimensions of the space in which all of the variables describing the velocity, momentum, rotation, mass for each of the several hundred particles is already an expansive one.  As they write, ‘it is evidently impossible to carry out a several hundred dimensional integral by the usual numerical methods, so we resort to the Monte Carlo method’ (1088), a method that Nicholas Metropolis and Stanislaw Ulam had several years previously already described in an earlier paper [@Metropolis_1949]. Here the problem is that the turbulent randomness of events in a square containing a few hundred particles thwarts calculations of the physical properties of the substance. They substitute for that non-integrable turbulent randomness a controlled flow of random variables generated by a computer. While still somewhat random (i.e. pseudo-random), these Monte Carlo variables taken together approximate to the integral of the many dimensional space. 

A toy example to show the intuition of Monte Carlo simulation is shown in Figure \ref{fig:pi_monte_carlo}. The point of this simulation, which comprises a half dozen lines of code, is to calculate the value of the mathematical constant $\pi$, a value that describes the ratio between the radius and circumference of a circle. In this Monte Carlo simulation of $\pi$, 100000 points are randomly generated, each point described by an x-y coordinate. Given the formula for the area of a circle ($\pi r^2$) and assuming the radius of the circle is 1 unit, the algorithm tests each random point to see if falls inside the circle. The ratio for $\pi$ is given by dividing the number of points inside the circle by the total number of randomly generated points and then multiplying by 4 ( since if the radius of the circle =1, then the diameter =2, and therefore, the total area of the bounding box = $2 \times 2$, so $\pi = 4 \times p$  the proportion inside the circle). The point of this demonstration is not to re-state the value of $\pi$, but to suggest that we can see here an inexact, probabilistic calculation of a number that epitomises mathematical precision and geometric ideal form (the circle). Monte Carlo simulation we might say puts ideal form  on a computational footing. 

```{r gibbs_normal_bivar, include=FALSE, echo=FALSE,  fig.cap= '', cache=TRUE, message=FALSE, warning=FALSE, comment=NA, dev='pdf'} 
  
    Niter=6*10^4
    v=1
    da = sample(c(rnorm(10^2), 2.5 + rnorm(4 * 10^2)))

    like = function(mu) {
        sum(log((0.2 * dnorm(da - mu[1]) + 0.8 * dnorm(da - mu[2]))))
    }

    mu1 = mu2 = seq(-2, 5, le = 250)
    lli = matrix(0, ncol = 250, nrow = 250)
    for (i in 1:250) 
      for (j in 1:250) 
        lli[i, j] = like(c(mu1[i],   mu2[j]))

    x = prop = runif(2, -2, 5)
    the = matrix(x, ncol = 2)
    curlike = hval = like(x)
    for (i in 2:Niter) {
        pp = 1/(1 + ((0.8 * dnorm(da, mean = the[i - 1, 2]))/(0.2 * dnorm(da, mean = the[i - 1, 1]))))
        z = 2 - (runif(length(da)) < pp)
        prop[1] = (v * sum(da[z == 1]))/(sum(z == 1) * v + 1) + 
            rnorm(1) * sqrt(v/(1 + sum(z == 1) * v))
        prop[2] = (v * sum(da[z == 2]))/(sum(z == 2) * v + 1) + 
            rnorm(1) * sqrt(v/(1 + sum(z == 2) * v))
        curlike = like(prop)
        hval = c(hval, curlike)
        the = rbind(the, prop)
    }

    image(mu1, mu2, -lli, col = topo.colors( 55 ), xlab = expression(mu[1]), ylab = expression(mu[2]))
    contour(mu1, mu2, -lli,drawlabels=FALSE,  nle = 50, add = T)
    points(the[1:100, 1], the[1:100, 2], cex = 0.2, pch = 17)
    min=0
    max= 50000
    points(the[min:max, 1], the[min:max, 2], cex = 0.8, pch=4)
    si = sample.int(dim(the)[1], 100)
    lines(the[si, 1], the[si, 2], cex = 0.6, pch = 19)
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/gibbs_normal_bivar-1.pdf}
        \caption{A Markov Chain Monte Carlo simulation of two normal distributions}
  \label{fig:gibbs_normal_bivar}
\end{figure}

The contour plot in Figure \ref{fig:gibbs_normal_bivar} was generated by a variant of Monte Carlo simulation called MCMC -- Markov Chain Monte Carlo simulation -- that has greatly transformed much statistical practices since the early 1990s (see [@Mcgrayne_2011] for a popular account). Like the simulation of $\pi$, this simulation calculates particular numbers, this time $\mu_1$ and $\mu_2$, the mean values of two probability distributions. This seemingly very simple simulation of the contours of two normally-distributed sets of numbers shows four main diagrammatic operations. First, pluri-dimensional fields arise at the intersection of different axes or vectors of variation. While the graphic plot here is two dimensional in this case, in can be algorithmically generalized to much higher dimensions, combining many more variables. Second, although Descartes may have first formalised the coordinate geometry using axes in the form of the Cartesian plane, we can see in Figure \ref{fig:gibbs_normal_bivar}  that this plane has a different consistency or texture. While the Cartesian axes are intact with their scales and marked intervals, the field itself is populated by a plural flux of random numbers   in the Monte Carlo simulation. The topographic convention of showing heights using contour lines work in Figure \ref{fig:gibbs_normal_bivar} to render visible continuously varying distributions of values across multi-dimensions. The curves of these contours derive  from large populations of random numbers generated in the simulation. While the curves of the contour lines join elevation that have the same value, the continuous undulation of values cannot be captured by any line or regular geometrical form.  Third, superimposed on the contour lines, a series of steps or a path explore that irregular topography of the pluri-dimensional field. This exploration appears as in the dense mass of black points on the figure deriving from the further flows of random numbers moving towards the highest point, the peak of the field, guided by continuous testing of convergence.  Finally, plot as a whole graphs  the different values of the means ($\mu_1, \mu_2$) of the variables distributed over a range of different possible values.  We need know nothing about what such variables relate to, apart from the fact they are something to do with probabilities, with assigning numbers to events  in some setting, whether they be the possible collisions of neutrons or the likelihood of an asthma attack. A set of connected points starting on the side of the one of the peaks and clustering on the peak shows how the MCMC algorithm explores the contours. Peaks -- zones that attract events or beliefs -- are sometimes difficult to find in complicated terrain. MCMC is a way of finding peaks.  

Compare this to a contemporary epidemiological model used by the Public Health Authority in the aftermath of the 2009 swine flu epidemic.  Here the peaks are also important, but now as a way of trying to sort out what in the course of an epidemic is due to force of infection of a virus and what is due to the social practices of people influenced by the widely publicised awareness of the onset of an epidemic. Here logistic supply chains of numbers deriving from doctors, pathology labs and population surveys are brought together to simulate in real time the trajectory of the pandemic. 

[^1.2]: Isabelle Stengers provides an excellent account of some of the nineteenth century development of thermodynamics [@Stengers_2011]. The history of statistics since the late seventeenth century obviously forms part of the background here [[@Stigler_1986; @Hacking_1975]. 


## Optimise in order to learn: 1957

We know from the histories of Cold War rationality that cognition and calculation are tightly entwined. Cold War cognition calculates its chances of winning, error, loss, costs, times and delays. But the mechanisms of this entwining of cognition and calculation are not necessarily obvious or homogeneous. Because of the prevalence of discursive analyses of power and knowledge, the subtle mechanisms and diagrammatic operations of cognitive calculation sometimes remain opaque. But it is precisely these mechanisms that flow along long fault-lines into contemporary knowledge apparatuses with all their power-generating armatures in areas such as security or online media. The operations of these mechanisms are often quite localised and in some cases trivial (e.g. fitting a straight line to some points), but their operations accumulate and generalise in ways that sometimes produce strategic, even hegemonic effects in contemporary culture. While there are quite a few operations that might be examined from this perspective, the case of the Perceptron from 1957 is evocative because of both its departure from cybernetic inspirations and its subsequent re-enactments during the 1980-1990s in the form of neural nets, in the very abundant support vector machines of the 1990s [@Cortes_1995], and today in the massively ramified form of 'deep learning' [@Hinton_2006]. 

'Learning' is pivotal to diagrammatic operation of machines such as the perceptron. While initially framed in terms of rational actors playing games (for instance in 1944 in Oskar Morgenstern and John von Neumann's _Theory of Games and Behaviour_ [@VonNeumann_2007]; see [@Erickson_2013, 133-134]), the locus of learning shifts diagonally through diagrams, plans and equations into different arrangements that had much less to do with the agential dilemmas of  Cold War military strategy.  While the perceptron retains some figurative aspects of its biological inspiration in the neurone, these figurative aspects are rapidly overlaid and displaced by asignifying processes that have continued to mutate in subsequent decades. The so-called 'learning problem' and the subsequent theory of learning machines was developed largely by researchers in the 1960-1980s, but based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@Rosenblatt_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network on a computer at the Cornell University Aeronautical Laboratory in 1957. A psychologist working in an aeronautical laboratory sounds rather odd, but given that the title of Rosenblatt's 1958 publication  -- 'The perceptron: A probabilistic model for information storage and organization in the brain' -- already suggested an intersection between statistics (probabilistic models), computation (information storage and organization), and neuroscience ('brain'), perhaps Rosenblatt's  cross-campus mobility is symptomatic of the diagonal movement occurring around learning. The very term 'perceptron' already amalgamates the organic and the organic, the psychological and the technological in a composite form. 

Like the Monte Carlo simulations, the perceptron operates according to a very simple intuition: a machine can learn by classifying things according to their position in a pluri-dimensional data space. 'Geometrically speaking,' writes Vladmir Vapnik (a machine learning researcher famous for his work on the support vector machine),  'the perceptron divides the space $X$ into two parts separated by a piecewise linear surface. ... Learning in this model means finding appropriate coefficients for all neurons using given training data' [@Vapnik_1999, 3]. If the Monte Carlo simulation generated a space in which many different variables could be integrated in exploring probability distributions, devices such as the perceptron configure a space in which classification takes place on a moving substrate of relations between data points. 

```{r perceptron, fig.cap='', include=FALSE, dev='pdf'}
    source('perceptron.r')
    eta  <- .01
    cols <- rainbow(10) # colors for visualization
    w    <- c(0,1,-1)
    m    <- matrix(c(2,4, 1,.5, .5,1.5, 0,.5), nrow=2)
    y    <- c(1,1,-1,-1)


    plotHeader(m, eta)
    trainPerceptron(m, y, w, eta)


```
\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/perceptron-1.pdf}
        \caption{Perceptron learns to separate}
  \label{fig:perceptron}
\end{figure}


Secondly, the role of learning. As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@Vapnik_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@Minsky_1969], later work showed that perceptrons could 'learn universally.' For present purposes, the key point is not that neural networks have turned out by the 1980s to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (for instance, in commercial applications such as drug prediction [@Dahl_2013]).  Rather, the important point is that it began to introduce learning machines as an ongoing project in which trying to understand what machines can learn, and to predict how they will classify or predict became central concerns precisely because machines didn't seem to classify or predict things at all well. At the same time, and less visibly,  implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines became more statistically sophisticated. 

The practice of learning here owes much more to logistics than it does to artificial intelligence in the classical sense. That is, learning occurs through and takes the form of optimisation. Optimisation in turn is understood in in terms mathematical functions located in high-dimensional spaces that cannot be analysed in closed-forms, but only explored looking for maxima or minima. Optimisation algorithms such as as gradient descent or expectation maximisation (EM) are the key components here. The difference between 
That is, the theory of machine learning alongside decision theory was interwoven with a set of concepts, techniques and language drawn from statistics. Just as humans, crops, habitats, particles and economies had been previously, learning machines became entwined with statistical methods. Not only in their reliance on the linear model as a common starting point, but in theories of machine learning, statistical terms such as bias, error, likelihood and indeed an increasingly thorough-going probabilistic framing of learning machines emerged. 

Learning machines optimise rather than cognize. The plot of a few points in a two dimensional space shown in Figure \ref{fig:perceptron} again has to stand in for a much more voluminous, densely populated and pluri-dimensional space. The different shapes of the points index different categories of things (for example, `male` vs. `female`). The lines in this figure are the work of a perceptron learning to classifying the points by searching for lines that divide the space. Starting with an arbitrary line, the perceptron tests whether a line effectively separates the different categories. If it does not cleanly separate them, the algorithm incrementally adjusts the parameters that define the slope and intercept until the line does run cleanly between the different categories. It easily may be that the different categories overlap, in which case, the perceptron algorithm will never converge since it cannot find any line that separates them.

For our purposes the point is that any learning that occurs here lies quite a long way away from biological figure of the neurone. Some biological language remains -- 'activation functions', for instance, figure in the code that produces the lines in  -- but the algorithmic process of testing lines and adjusting weights in order to find a line or plane or hyperplane (in higher dimensional data) very definitely an optimisation process in which errors are gradually reduced to a minimum. Furthermore, the diagrammatic operation of the perceptron  -- repeating drawing of lines in order to classify -- appears in a number of variations in the following decades. Some of these variations -- linear discriminant analysis, logistic regression, support vector machine -- generalise the process of finding separating lines or planes in data in quite complicated ways in order to find more supple or flexible classifications. The perceptron is not unique in doing this. Subsequent developments, including various machine learning models such as logistic regression, neural nets, support vector machines, _k_ nearest neighbours and others present variations on the same diagrammatic operation: pattern recognition entails learning to classify; learning to classify means finding a way of best separating or partitioning  a pluri-dimensional data space; the best partition optimises by reducing the number of misclassification errors. It is not surprising that later iterations and variations of the perceptron proliferated the process of optimisation. For instance, neural nets, current favourite machine learning techniques for working with large archives of image and sound, aggregate many perceptrons in layered networks, so that the perceptron's diagrammatic operation of classification can be generalised to highly complex patterns and shapes in the field of data. No matter how complex the classifiers become, they still  propagate the same diagrammatic operation of drawing a line and testing its power to separate. 


## Ramified movements

The final diagrammatic operation concerns decisions. Cold War information theory says information is that which allows a move down a decision tree [@Solovey_2012, 103]. Decisions -- as the term itself suggests -- are a kind of cut. But decisions often have a complicated branching structure, at least, in the procedural rationality typical of Cold War operations research and logistics. In procedural rationality, branches in a decision tree stem from rules that embody the expert knowledge of the grey eminences of management science and its cognate power-knowledge formations.  Increasingly during the decade of the Cold War, these rules were shaped by optimisation and modelling procedures  that sought to allocate resources most efficiently (especially in the field of operations research; see [@Erickson_2013, 79]). 

The decision tree was and remains a key epistemic construct in Cold War closed-world thinking in its attempt to represent optimal allocation of resources. The development of procedural rationality based on various algorithmic procedures (linear programming, dynamic programming [@Bellman_1961]) was however paralleled in pattern recognition and machine learning by a different form of decision tree: the classification and regression tree [@Breiman_1984] or recursive partitioning.  During the 1960s, the decision tree itself is reorganised in data mining and pattern recognition into a rather different kind of device that in some ways owes more to the systems of classification associated with taxonomy or natural history. The decision tree is no longer a way of regulating information flow towards optimum resource allocation (missiles, cargoes, troops, etc.). In classification and regression trees, branches are  instead something to be learned from the data rather than from experts or their optimization techniques. Decision rules are replaced by learning algorithms that partition according to quasi-statistical measures of mixedness or purity. To give a brief indication of how the decision tree has changed its mode of operation, we might think of the Microsoft Kinect game controller, a popular gaming interface that uses video cameras and decision tree algorithm to learn to classify players' gestures and movements, and thereby allow them to play a computer game without touching buttons, levels or holding a game controller. The decision algorithm, coupled with the imaging system, translates the mobility and fluidity of gesture into a set of highly coded movements in the on-screen space of the game. 

In a certain sense, the decision tree (and its contemporary incarnation in the very popular random forest [@Breiman_2001]) dismantles the classical tree with its reference to kinds of being. It also obviates in certain ways the decision tree of procedural rationality as a distillation of expert knowledge. And finally, it potentially transforms the biopolitical rendering of differences through specific attributes of individual and populations into a rather mobile matrix of potential mixtures and overlaps. Take the case of the `iris` dataset, one of the most famous datasets in the machine learning scientific literature. The eugenicist statistician Ronald A. Fisher first used this dataset in his work on the important linear discriminant analysis technique in the late 1930s [@Fisher_1938]. The `iris` in some ways innocuously enough epitomises the modelling of species differences via measurements of biological properties (in this, measurements of such things as petal widths and lengths of irises growing in the Gaspé Peninsula in Canada). 

```{r decision_tree, echo=FALSE, include=FALSE, dev='pdf'}
library(ggplot2)
library(tree)
data(iris)
tree1 <- tree(Species ~ Sepal.Width + Petal.Width,  data = iris)
par(mfrow = c(1,2))
plot(tree1)
text(tree1)
partition.tree(tree1,label="Species",add=TRUE)
plot(iris$Petal.Width,iris$Sepal.Width, cex=0.65,  col=as.numeric(iris$Species), pch = as.numeric(iris$Species))
legend(1.75,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
```

\begin{figure}
  \centering
      \includegraphics[width=0.99\textwidth]{figure/decision_tree-1.pdf}
        \caption{Decision tree model on `iris` data}
  \label{fig:iris_tree}
\end{figure}


The plot on the left in Figure \ref{fig:iris_tree} shows the decision tree and the plot on the right shows the three _iris_ species _virginica_,  _setosa_ and _versicolor_  plotted by petal and sepal widths.  As the plot on the right shows, most of the measurements are well clustered when plotted. Only the _setosa_ petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. This means that the decision tree shown on the left has little trouble classifying the irises. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is _setosa_. Hastie, Tibshirani and Friedman  suggest that 'a key advantage of the recursive binary tree is its interpretability. The feature space partition is fully described a by single tree.  ... This representation is popular among medical scientists, perhaps because it mimics the way a doctor thinks.  The tree stratifies the population into strata of high and low outcome, on the basis of patient characteristics' [@Hastie_2009, 306-7]. I would differ from them on this point.  Decision trees do indeed have a rich  medical, as well  commercial and industrial history of use. And yes, decision trees and their later variations (such  as`C4.5`, the 'top' data-mining algorithm according to a survey of data miners in 2009 [@Wu_2008] was developing during the 1980s in response to Friedman's work on decision trees) are often presented as easy to use because they are 'not unlike the serious of troubleshooting questions you might find in your car's manual to help determine what could be wrong with the vehicle' [@Wu_2008,2].  While that scenario is unlikely today, especially as Google sends autonomous-driving cars out onto the roads of California undoubtedly controlled by a variety of classifiers such as decision trees, neural networks and support vector machines, the recursive partitioning technique still has a great deal of traction in machine learning practice precisely because of its simplicity. 

In  `iris`, species differences have an ontological or biopolitical weight. The pattern of separation that the decision tree algorithm finds also exists in the world for us. But the recursive partitioning decision tree knows nothing of this difference. Moreover, in many cases, things  are not cleanly separable even for experts. Often there is some pattern of separation, perhaps in the form of overlapping clusters or clouds of points, but not enough to define a simple set of decision rules.   How then does a decision tree decide how to split things? What counts as a good split has been a long standing topic of debate in the decision tree literature. Choosing where to cut: this is a key problem for classification or decision trees. As the statisticians Malley, Malley and Pajevic observe, 'the challenge is to define _good_ when its clear that no obviously excellent split is easily available' [@Malley_2011, 121]. Regardless of how 'excellent splits' are defined, the recursive partitioning decision tree introduces new flows and patterns of flow of information into decision. Decision trees, especially in their more recent incarnations of ensembles of decision trees (for example, random forests), no longer seek to formally express the rules that Cold War rationality used to stabilise power-knowledge. The generation of the rules that define the branches in diagrams such as Figure \ref{fig:iris_tree} no longer relies on domain experts. Decisions diverge somewhat from cognitive skill or technical rationality to reappear as recursive generated partitions. There is little human gesture expertise implemented in the Kinect game controller, only algorithms that learn  to classify gestures into a number of categories by propagating gestural date down a decision tree. These categories still refer to conventions and codings of the world, but algorithmically these classifications have little ontological depth, since they only derive from relative density and sparsity in the data. 

We are dealing here with classificatory operations that differ quite radically from either the artificial intelligence of the Cold War with its attempts to develop computer intelligence, or the expert decision support systems of the 1970s and 1980s that sought to capture domain expertise in software [@Collins_1990]. Both still assumed that there could be in principle separation or discrimination underpinning decisions and classifications and that algorithmic operations should learn to mimic the discriminative finesse of experts. Cold War rationality remained attached to a decisionistic logic that classification and regression trees, even if only by their sheer abundance, radically re-scales. The branches of the trees multiply, but the bifurcations of the trees derive from the learning algorithms that cut according to measures of mixture. 

## Conclusion: the pluri-dimensional space

Monte Carlo simulations and the subsequent Markov Chain Monte Carlo simulations, the perceptron and its ramification in contemporary neural nets, or the decision tree and its coppicing in random forests and similar ensembles are just some of the matrices of transformation playing out in contemporary science, media and government. These are just some examples of operations flowing through a vast and complex space of scientific knowledges, techniques, technologies, infrastructures, and disciplines in which pattern and the production of pattern. They are hundreds of other techniques in these spaces, and literally thousands of implementations and variations: Gaussian mixture models, gradient boosted trees, penalized logistic regression, AdaBoost, expectation maximization, linear discriminant analysis, topic models, principal component analysis, independent component analysis, etc. (see [@Hastie_2009] for a reasonably comprehensive textbook listing).

While the exemplary diagrams I have discussed do not exhaust the spectrum of movements occurring in the pluri-dimensional space of data, they do point to the some of the principal axes along which many contemporary power-knowledges move as they search for hidden patterns and value in data. What happens to pattern in this power-knowledge nexus? A. N. Whitehead proposed that quantity presupposes pattern:

>Thus beyond all questions of quantity, there lie questions of pattern, which are essential for the understanding of nature. Apart from a presupposed pattern, quantity determines nothing. Indeed quantity itself is nothing other than analogy of functions within analogous patterns [@Whitehead_1956, 195].

I have been suggesting that we are experiencing a re-patterning of pattern today through pattern recognition. Each of the diagrammatic operations described above comprehends a proliferation of data and various enumerations that can be quantified. Text, voice, image, gesture, measurement, transaction, and many forms of record and recording have and are being ingested by digital systems as quantities. But the quantities or numbers involved presuppose pattern. The promise of pattern recognition, machine learning or data mining has been predicated on finding patterns in data rendered as number, but the production of data as number, and as massive accumulation of numbers might already derive from the shifts in seeing, differentiating and classifying that pattern recognition, data mining and machine learning introduce to the presupposed patterns. If algorithmic operations do locate patterns in data, this location already presupposes certain kinds of pattern. The differences between Monte Carlo simulation, the perceptron and the decision tree starkly delineate presupposed patterns that guide the relations between quantity that algorithms uncover, optimise or converge towards. I have framed these differences as diagrammatic operations to highlight their dependence on and inherence to criss-crossing visual, semiotic, mathematical, technical, and infrastructural planes.  All of these operations have somewhat diagonal tendencies, which project them across disciplinary boundaries (from physics to gaming media, from psychology to weapons development, from mathematical theory to handheld devices) with sometimes remarkable transcontextual momentum. The diagonal tendencies of, for instance, the decision tree with its indifference to the qualifications of quantity -- it traverses different kinds of data very easily -- differ from those of the Monte Carlo simulation with its intensive sampling of data spaces generated by accumulated random numbers.

Many different processes and decisions depend increasingly on such diagrammatic operations. If pattern itself takes on a new diagrammatic force, if its asignifying assimilation of differences to what Whitehead terms an 'analogy of functions within analogous patterns,' then it is because the Cold War problems of simulation, optimisation and classification find themselves concatenated in a new configuration. The few hundred neutrons of Metropolis and his co-authors expand to included hundreds of millions of observations of players on XBox-Live; the few hundred scientific articles classified by Maron's early decision tree [@Morgan_1963] expand to include several billion DNA base pairs of a cancer genome whose associations are analysed by random forests at a Google I/O conference demonstration [@Mackenzie_2015b]; the simple logical functions that almost choked the development of perceptrons in the 1960s are inundated by the billions of features that deep learning nets at Youtube and Yahoo pipeline into unsupervised object recognition tasks in online video. 

In this ramifying re-distribution of pattern, we can expect transformations of and reassignments of subject positions as once quite localised force-relations become strategies generalized to business, government and science. What counts as individual, what counts as population, who or what decides, and what the terms of decisions are, potentially shift or are re-classified in this generalization of the diagrammatic operations. Since their inception in problems of nuclear weapons design, logistics or cybernetics, techniques flow out of the closed-world spaces of the Cold War labs and research facilities. They become banal devices rather than instruments of a decisionistic elite. In this movement,  another space takes shape, a space whose dimensions are practically treated as open-ended, and whose potential expansion animates the massive build out of infrastructures and the intense efforts to scale up and scale down circuitry and electronic devices. We might need to think about how it might be possible to inhabit this space of patterns as these patterns become power matrices of transformation. The three general cases discussed above all suggest ongoing instability in what counts as pattern, and how pattern derives from movements through data. 

## References


