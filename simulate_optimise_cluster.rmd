## Simulate, optimise, cluster: the algorithmic organisations of pluri-dimensional space from 1953 onwards

## todo

- get Halpern
- get erickson
- add in Fuller
- add in Boyd



## Introduction

Contemporary attempts to find patterns in data, ranging from the now mundane technologies of hand-writing recognition through to mammoth infrastructure-heavy practices of deep learning conducted by major business and government actors to find cats [@Markoff_2012], credit card fraud or terrorists, rely on  group of techniques intensively developed during the 1950-60s in physics, engineering and psychology.   Whether we designate them as pattern recognition, data mining or machine learning (all terms that first came into play during the 1950s), proponents of these techniques all seek to uncover patterns in data that cannot appear directly to the human eye, either because there are too many items for anyone to look at, or because the patterns are too subtly woven through in the data. This at least is the standard contemporary narrative of their efficacy and indeed necessity. From the plethora of techniques in current use, three techniques developed in the Cold War era iconify contemporary modes of pattern finding: Monte Carlo simulation as a way of shaping flows of random numbers was developed to calculate neutron fluxes in atomic fission; gradient descent as a way of finding numerical solution to systems of equations that cannot be solved analytically was applied to problems of optimisation in many engineering settings; and finally clustering algorithms that search for groups or clusters in data proliferated in psychological, social and field sciences. Each of these techniques implements a different mode of mapping and partitioning differences in data. These different modes of mapping and partitioning flow through into contemporary scientific, technological, business and governmental problematizations. The different perspectives on event, trajectory, and proximity they embody imbue many power relations, forms of value and the play of truth/falsehood today. 

> If the operatives of the Cold War could reserve for themselves the position of gray eminence, the distant advisor to the executive power, the new spaces of collectively intelligent networks and the asymmetrical relations these put in place demand instead the more difficult position of gray immanence [@Fuller_2012, 32].

The shift between grey eminence or powerful advisors to executive power and grey immanence in intelligent networks is precisely the movement that we might delineate by paying attention to the abstract machines of simulation, optimisation and clustering that underpin and in many ways epitomise the tain of social network media, contemporary health and biomedical knowledges and credit ratings, to name a few. By looking at certain techniques for working with data, number, probability and pattern that took shape deep in the epistemic cultures of the Cold War, we can begin to see the emergence of an expansive space, a space that is neither that of the lifeworld (lived, urban, etc) nor a mathematical abstraction (Euclidean space), nor even a single assemblage, but something more like a problematisation in Michel Foucault's sense of the term. The dimensionality and scale of this space undergoes constant expansion and contraction, partitioning and aggregation. Through it, in it, near it, derived from it, many different artifices, devices, arrangements, operations and processes emerge. It is power-laden space that traverses and structures many aspects of our lives, but is only intermittently sensible or palpable to us. I don't yet know what to call this largely hidden space – I'm interested in how to name it. It appears at the intersection of many scientific disciplines, various infrastructures, techniques, institutions. It is object and domain of much work and investment in management, enterprise and State. 



