## Simulate, optimise, partition: algorithmic organisations of the pluri-dimensional field from 1953 onwards

## todo

- get Halpern
- add in Boyd
- bowker on cybernetic strategy



## Introduction

Contemporary attempts to find patterns in data, ranging from the now mundane technologies of touchscreen gesture recognition through to mammoth infrastructure-heavy practices of deep learning conducted by major business, scientific and government actors to find cats [@Markoff_2012], the Higgs boson, credit card fraud or terrorists, rely on  group of techniques intensively developed during the 1950-60s in physics, engineering and psychology.   Whether we designate them as pattern recognition, data mining or machine learning (all terms that first came into play during the 1950s), the standard account enunciated by proponents (and opponents) of these techniques is that they uncover patterns in data that cannot appear directly to the human eye, either because there are too many items for anyone to look at, or because the patterns are too subtly woven through in the data. This at least is the standard contemporary narrative of their efficacy and indeed necessity. The spectrum of differences accommodated under the rubric of pattern is striking. Data here is understood to encompass language, images, measurements and traces of many different kinds. These techniques (although that term is problematic because it suggests that skilled hands doing something; I will refer to them as _operations_) construct a new kind of continuum or field which accommodates seemingly very different things -- terrorists, fundamental particles, photographs, utterances and gestures -- more or less uniformly. 
        
From the plethora of operations in current use, three operations developed in the Cold War era iconify contemporary modes of pattern finding: Monte Carlo simulation as a way of shaping flows of random numbers was developed to calculate neutron fluxes in atomic fission; gradient descent as a way of finding numerical solutions -- convex optimisations -- to systems of equations that cannot be solved analytically was applied to problems of optimisation in many engineering settings; and finally classifier algorithms that locate groups or clusters in data proliferated in psychological, social and field sciences. Each of these operations configurates a different mode of moving through data. These different modes of mapping and partitioning flow through into contemporary scientific, technological, business and governmental problematizations. The different perspectives on event, trajectory, and proximity they embody imbue many power relations, forms of value and the play of truth/falsehood today. In each case, the operation contributed something to the operational field that todday that no longer exists purely or in silation, but in combination with each other.


## What are continuities in the operational field?

> If the operatives of the Cold War could reserve for themselves the position of gray eminence, the distant advisor to the executive power, the new spaces of collectively intelligent networks and the asymmetrical relations these put in place demand instead the more difficult position of gray immanence [@Fuller_2012, 32].

The shift between grey eminence or powerful advisors to executive power and grey immanence in intelligent networks is precisely the movement that we might delineate by paying attention to the operations of simulation, optimisation and partitioning that underpin and in many ways epitomise the tain of social network media, contemporary health and biomedical knowledges and credit ratings, to name a few. By looking at certain operations for working with data, number, probability and pattern that took shape deep in the epistemic cultures of the Cold War, we can begin to see the emergence of an expansive field, a field that is neither that of the lifeworld (lived, urban, etc.) linked to a subject position (the grey eminences of Cold War science, as described for instance in _How Reason Almost Lost Its Mind_ [@Erickson_2013]  nor a mathematical abstraction (Euclidean space), nor even a single assemblage, but something more like an _operational field_ in Michel Foucault's sense of the term [@Foucault_1972, 106].[^1.1] The dimensionality and scale of this field undergoes constant expansion and accumulation, partitioning and aggregation. Through it, in it, near it, derived from it, many different artifices, devices, arrangements, operations and processes accumulate. It is an power-laden operational space that traverses and structures many aspects of our lives, but is only intermittently sensible or palpable to us. It appears at the intersection of many scientific disciplines, various infrastructures, operations, and institutions. It is object and domain of much work and investment in management, enterprise and State. 

[^1.1]: Foucault writes:

> I now realize that I could not define the statement as a unit of a linguistic type (superior to the phenomenon of the word, inferior to the text); but that I was dealing with an enunciative function that involved various units (these may sometimes be sentences, sometimes propositions; but they are sometimes made up of fragments of sentences , series or tables of signs, a set of propositions or equivalent formulations); and, instead of giving a 'meaning' to these units, this function relates them to a field of objects; instead of providing them with a subject, it opens up for them a number of possible subjective positions; instead of fixing their limits, it places them in a domain of coordination and coexistence; instead of determining their identity, it places them in a space in which they are used and repeated. In short, what has been discovered is not the atomic statement - with its apparent meaning, its origin, its limits, and its individuality - but the operational field of the enunciative function and the conditions according to which it reveals various units (which may be, but need not be, of a grammatical or logical order) [@Foucault_1972,  106]

The composition of this field is distributed across methods, operations, infrastructures, forms of expertise, models. It loosely coalesces along some tensors of dense coherence that the current examples highlight.  We can't understand the force of the contemporary operational field without moving backwards and seeing how those power-laden operations proliferated and coalesced. 

## Exact means simulated, simulated means open

In 1953, Nicholas Metropolis, the Artur and TBA Rosenbluths, Edward and TBA Teller, all physicists working at Los Alamos,  were considering ‘the properties of any substance which may be considered as composing of interacting individual molecules’ [@Metropolis_1953, 1087]. These properties might be, for instance, the flux of neutrons in a hydrogen bomb detonation. [@Metropolis_1949 In their short, evocatively titled and still widely cited paper 'Equations of state for fast calculating machines' (over 20,000 citations according to Google Scholar; over 14,000 according to Thomson Reuters Web of Knowledge), they describe how they used computer simulation to manage with the inordinate number of possible interactions in a substance, and to thereby come up with a statistical description of the properties of the substance. While statistical descriptions of the properties of things are not new, [^1.2]  their model system consists of a square containing only a few hundred particles. (This space is a typical multivariate joint distribution.) These particles are at various distances from each other and exert forces (electric, magnetic, etc.) on each other dependent on the distance. In order to estimate the probability that the substance will be in any particular state (fissioning, vibrating, crystallising, cooling down,  etc.), they needed to integrate over the many dimensional space comprising all the distance and forces between the particles. The dimensions of the space in which all of the variables describing the velocity, momentum, rotation, mass for each of the several hundred particles is already an expansive one.  As they write, ‘it is evidently impossible to carry out a several hundred dimensional integral by the usual numerical methods, so we resort to the Monte Carlo method’ (1088), a method that Nicholas Metropolis and Stanislaw Ulam had several years previously already descibed in an earlier paper [@metropolis_monte_1949]. Here the problem is that the turbulent randomness of events in a square containing a few hundred particles thwarts calculations of the physical properties of the substance. They substitute for that non-integrable turbulent randomness a controlled flow of random variables generated by a computer. While still somewhat random (i.e. pseudo-random), these Monte Carlo variables taken together approximate to the integral of the many dimensional space. In monadological terms, Monte Carlo simulation attenuates the distance between the aleatory and epistemic poles of probability. While the computer is regarded as aleatory in its capacity to generate seemingly random numbers, it is strongly epistemic in its power to marshall these numbers into shapes that cannot be analysed using the 'usual numerical methods' (for instance, of integral calculus). 

The contour plot in Figure 1 was generated by a  statistical simulation technique called MCMC -- Markov Chain Monte Carlo simulation -- that has greatly transformed much statistical practices since the early 1990s (see [@mcgrayne_theory_2011] for a popular account). This very simple simulation of the contours of two normally-distributed sets of numbers shows two main things. The contour lines trace  the different values of the means ($\mu_1, \mu_2$) of the variables. For the time being, we need know nothing about what such peaks refer to, apart from the fact they are something to do with probability, with assigning numbers to events. A set of connected points starting on the side of the one of the peaks and clustering on the peak shows how the MCMC algorithm explores the contours. Peaks -- zones that attract events or beliefs -- are sometimes difficult to find in complicated terrain. MCMC is a way of finding peaks.  
Compare this to a contemporary epidemiological model used by the Public Health Authority in the aftermath of the 2009 swine flu epidemic.  Here the peaks are also important, but now as a way of trying to sort out what in the course of an epidemic is due to force of infection of a virus and what is due to the social practices of people influenced by the widely publicised awareness of the onset of an epidemic. Here logistic supply chains of numbers deriving from doctors, pathology labs and population surveys are brought together to simulate in real time the trajectory of the pandemic. 

[^1.2]: Isabelle Stengers provides an excellent account of some of the nineteenth century development of thermodynamics [@Stengers_2011]. The history of statistics since the late seventeenth century obviously forms part of the background here [[@Stigler_1986; @Hacking_1975]. 







## Conclusion



## References


