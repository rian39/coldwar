## Simulate, optimise, cluster: the algorithmic organisations of pluri-dimensional space from 1953 onwards

## Introduction

Contemporary attempts to find patterns in data, ranging from the now mundane technologies of hand-writing recognition through to mammoth infrastructure-heavy practices of deep learning conducted by major business and government actors to find cats [@Markoff_2012], credit card fraud or terrorists, rely on group of techniques intensively developed during the 1950-60s in physics, engineering and psychology.   Whether we designate them as pattern recognition, data mining, or machine learning, these techniques all seek to uncover patterns in data that cannot appear directly to the human eye, either because there are too many items for anyone to look at, or because the patterns are too subtly woven through in the data. From the plethora of techniques in current use, three techniques developed in the Cold War era iconify contemporary modes of pattern finding: Monte Carlo simulation as a way of sampling random events was developed to calculate neutron fluxes in atomic fission; gradient descent as a way of finding the solution to systems of equations that cannot be solved analytically was applied to problems of optimisation in many engineering settings; and finally clustering algorithms that search for groups or clusters in data proliferated in psychological, social and field sciences. Each of these techniques implements a different mode of pattern, and these different modes of pattern recognition flow through into contemporary scientific, technological, business and governmental problematizations. The different perspectives on event, trajectory, and proximity they embody imbue many power relations, forms of value and the play of truth/falsehood today. 
Paper is about transformations in computation, the premiere technical activity of Cold War science, politics and culture.  Today when computability is really so taken for granted, and yet still so unsettled, continuities and discontinuities might help us make sense of how ongoing dynamisms are engineered into lives. 
My suggestion is by looking at certain techniques for working with data, number, probability and pattern that took shape deep in the epistemic cultures of the Cold War, we can begin to see the emergence of an expansive space, a space that is neither that of the lifeworld (lived, urban, etc) nor a mathematical abstraction (Euclidean space), nor even a single continuous domain. The dimensionality and scale of this space undergoes constant expansion and contraction, partitioning and aggregation. Through it, in it, near it, derived from it, many different artifices, devices, arrangements, operations and processes emerge. It is power-laden space that traverses and structures many aspects of our lives, but is only intermittently sensible or palpable to us. I don't yet know what to call this largely hidden space – I'm interested in how to name it. It appears at the intersection of many scientific disciplines, various infrastructures, techniques, institutions. It is object and domain of much work and investment in management, enterprise and State. 

## What are the continuities and what are the discontinuities?

The composition of this space is distributed across methods, techniques, infrastructures, forms of expertise, models. It loosely coalesces along some tensors of dense coherence that the current examples highlight.  But we can't understand the force of the contemporary problematisation without moving backwards and seeing how those power-laden techniques proliferated and coalesced. 

Three examples of morphing continuity in techniques

## Prediction: exact means simulated, simulated means open: MCMC

1. From Ulam, Rosenbluth, Tellers working at Los Alamos to Swine Flu London 2009 – connecting people – computing probability distributions through to connecting
In 1953, Metropolis, the Rosenbluths and and the Tellers, all physicists working at Los Alamos,  were calculating ‘the properties of any substance which may be considered as composing of interacting individual molecules’ [@metropolis_equation_1953, 1087]  (for instance, the flux of neutrons in a hydrogen bomb detonation). In their short, but still widely cited paper (over 20,000 citations according to Google Scholar; over 14,000 according to Thomson Reuters Web of Knowledge), they describe how they used computer simulation to deal with the number of possible interactions in a substance, and to thereby come up with a statistical description of the properties of the substance. Their model system consists of a square containing only a few hundred particles. These particles are at various distances from each other and exert forces (electric, magnetic, etc.) on each other dependent on the distance. In order to estimate the probability that the substance will be in any particular state (fissioning, vibrating, crystallising, cooling down,  etc.), they needed to integrate over the many dimensional space comprising all the distance and forces between the particles. (This space is a typical multivariate joint distribution.) As they write, ‘it is evidently impossible to carry out a several hundred dimensional integral by the usual numerical methods, so we resort to the Monte Carlo method’ (1088), a method that Nicholas Metropolis and Stanislaw Ulam had already descibed in an earlier paper [@metropolis_monte_1949]. Here the problem is that the turbulent randomness of events in a square containing a few hundred particles thwarts calculations of the physical properties of the substance. They substitute for that non-integrable turbulent randomness a controlled flow of random variables generated by a computer. While still somewhat random (i.e. pseudo-random), these Monte Carlo variables taken together approximate to the integral of the many dimensional space. In monadological terms, Monte Carlo simulation attenuates the distance between the aleatory and epistemic poles of probability. While the computer is regarded as aleatory in its capacity to generate seemingly random numbers, it is strongly epistemic in its power to marshall these numbers into shapes that cannot be analysed using the 'usual numerical methods' (for instance, of integral calculus). 
The contour plot in Figure 1 was generated by a  statistical simulation technique called MCMC -- Markov Chain Monte Carlo simulation -- that has greatly transformed much statistical practices since the early 1990s (see [@mcgrayne_theory_2011] for a popular account). This very simple simulation of the contours of two normally-distributed sets of numbers shows two main things. The contour lines trace  the different values of the means ($\mu_1, \mu_2$) of the variables. For the time being, we need know nothing about what such peaks refer to, apart from the fact they are something to do with probability, with assigning numbers to events. A set of connected points starting on the side of the one of the peaks and clustering on the peak shows how the MCMC algorithm explores the contours. Peaks -- zones that attract events or beliefs -- are sometimes difficult to find in complicated terrain. MCMC is a way of finding peaks.  
Compare this to a contemporary epidemiological model used by the Public Health Authority in the aftermath of the 2009 swine flu epidemic.  Here the peaks are also important, but now as a way of trying to sort out what in the course of an epidemic is due to force of infection of a virus and what is due to the social practices of people influenced by the widely publicised awareness of the onset of an epidemic. Here logistic supply chains of numbers deriving from doctors, pathology labs and population surveys are brought together to simulate in real time the trajectory of the pandemic. 
PUT PLOTS FROM PAPER HERE

> What MCMC has added to the world is subtle yet indicative. In a history of the technique, Christian Robert and George Casella, two leading statisticians specializing in  MCMC,  write that  

‘Markov chain Monte Carlo changed our emphasis from “closed form” solutions to algorithms, expanded our impact to solving “real” applied problems and to improving numerical algorithms using statistical ideas, and led us into a world where “exact” now means “simulated”’ [@robert_history_2008,18].  
Monte Carlo simulations render computers as substitutes for events in the world, and they render that world more manipulable by knowing subjects. It is hardly surprising that scientists working at the epicentre of the ‘closed world’ [@edwards_closed_1996] of post-WWII nuclear weapons research should develop a technique that allows the world to move in this way. 
It has often been mentioned that simulations were at the heart of the Cold War (Edwards, 1996), Simulation but not of a thing, but of events that can't be described analytically because they are too random in some way. Monte Carlo simulations. Convolution of simulation in order to incorporate experience or events. Shift from Ulam's bomb to TrueSkill  – reliance on randomness in both, but reshaped massively. Massive and ongoing injections of randomness. Not all at once in the 1950s – there were successive waves of this occurring around different – late 70s, early 80s, mid-90s; etc.
IN 2010, MCMC was voted into the top 10 algorithms by data miners.

## Optimise in order to classify: 1957

2. From Perceptron  to Kittydar/Google Deep Learning – from classifying things in images – putting large numbers of perceptrons together
The so-called 'learning problem' and the theory of learning machines was developed largely by researchers in the 1960-1970s, based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@rosenblatt_perceptron_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network on a computer at the Cornell University Aeronautical Laboratory in 1957. 
Geometrically speaking, the perceptron divides the space $X$ into two parts separated by a piecewise linear surface. ... Learning in this model means finding appropriate coefficients for all neurons using given training data. (Vapnik, 1999, p. 3) 
SHOW PYTHON EXAMPLE HERE
SHOW Kittydar here

There are two features we might extract here. 

Firstly, the very term 'perceptron' already almalgamates the organic and the organic, the psychological and the technological in a composite form. This was typical cybernetic discourse, as Bowker tells us (Bowker, 1993). Symbiosis of living and non-living.

Secondly, the role of learning. As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@vapnik_nature_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@minsky_perceptron:_1969], later work showed that perceptrons could 'learn universally.' Learning universally meant  For present purposes, the key point is not that neural networks have turned out by the 1980s to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (see for instance, for their use in sciences [@hinton_reducing_2006], or in commercial applications such as drug prediction [@dahl_deep_2012]).  Rather, the important point is that it began to introduce learning machines as an ongoing project in which trying to understand what machines can learn, and to predict how they will classify or predict became central concerns precisely because machines didn't seem to classify or predict things at all well. At the same time, and less visibly,  implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines became more statistically sophisticated. 

The practice of learning here owes much more to logistics than it does to artifical intelligence in the classical sense. That is, learning occurs through and takes the form of optimisation. Optimisation in turn is understood in in terms mathematical functions located in high-dimensional spaces that cannot be analysed in closed-forms, but only explored looking for maxima or minima. Optimisation algorithms such as as gradient descent or expectation maximisation (EM) are the key components here. The difference between 
That is, the theory of machine learning alongside decision theory was interwoven with a set of concepts, techniques and language drawn from statistics. Just as humans, crops, habitats, particles and economies had been previously, learning machines became entwined with statistical methods. Not only in their reliance on the linear model as a common starting point, but in theories of machine learning, statistical terms such as bias, error, likelihood and indeed an increasingly thorough-going probabilistic framing of learning machines emerged. 

## 1957: Optimisation: perceptron as borrowing from cybernetics, and exemplification of how cybernetics makes no distinction between organism and machine, living and non-living. Here the cognitive act of classification undertaken by a mechanism. But what shifts here is the engineering of these devices as abstractions or mathematical functions that are to be optimised by training them and working out how to train them by back-propagation  – artificial neural networks (1980s).  Learning becomes optimising. This was already part of the military (Bousquet, 2008, p. 78) But this Breaks and discontinuities  – the Minsky thing; 

## Clustering – there are no decision trees

3. From decision rules to rf-ace and cancer genome GoogleCompute – connecting people 
In Cold War, information theory says information is that which allows a move down a decision tree (Solovey and Cravens, 2012, p. 103). The decision tree was and remains a key epistemic construct closed-world thinking. But the decision tree itself is reorganised by the classification and regression tree as something to be learned. Decision rules replaced by learning algorithms that partition according to quasi-statistical measures of mixedness or purity. That is, what if there is no decision tree, no rationality or clear and distinct notions that carve the world?
SHOW Google Compute engine demo
https://www.youtube.com/watch?v=IheEApKdgC8
The plot on the left shows the decision tree and the plot on the right shows just _setosa_ and _versicolor_  plotted by petal and sepal widths and lengths.  As the plot on the right shows, most of the measurements are well clustered. Only the _setosa_ petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. This means that the decision tree shown on the left has little trouble classifying the irises. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is _setosa_. Hastie, Tibshirani and Friedman  suggest that 'a key advantage of the recursive binary tree is its interpretability. The feature space partition is fully described a by single tree.  ... This representation is popular among medical scientists, perhaps because it mimics the way a doctor thinks.  The tree stratifies the population into strata of high and low outcome, on the basis of patient characteristics' [@hastie_elements_2009, 306-7]. I would differ from them on this point.  Decision trees do indeed have a rich  medical, as well  commercial and industrial history of use. And yes, decision trees and their later variations (such  as`C4.5`, the 'top' data-mining algorithm according to a survey of data miners in 2009 [@wu_top_2008] was developing during the 1980s in response to Friedman's work on decision trees) are often presented as easy to use because they are 'not unlike the serious of troubleshooting questions you might find in your car's manual to help determine what could be wrong with the vehicle' [@wu_top_2008,2]. (But who actually reads car manuals?) While that scenario is unlikely today, especially as Google sends autonomous-driving cars out onto the roads of California undoubtedly controlled by a variety of classifiers such as decision trees, neural networks and support vector machines, the recursive partitioning technique still has a great deal of traction in machine learning practice precisely because of its simplicity. 
The problem with _iris_ is that the species actually ontically separate. The pattern of separation that the decision tree algorithm finds exists in the world for us too, except when, as happens, species of iris hybridise with each other. In many cases, things as Whitehead suggests, are not cleanly separable. Often there is some pattern of separation, perhaps in the form of overlapping clusters or clouds of points, but not enough to define a simple set of decision rules.  (As we will see that support vector machines, with their 'maximum margin hyperplanes,' take these overlaps as given, and build models focused on the overlaps.) How then does a decision tree decide how to split things? What counts as a good split has been a long standing topic of debate in the decision tree literature. Choosing where to cut: this is a key problem for classification or decision trees. As Malley, Malley and Pajevic observe, 'the challenge is to define _good_ when its clear that no obviously excellent split is easily available' [@malley_statistical_2011, 121].  The definition of 'good' that has emerged in the decision tree literature is centred on 'node purity.'

We are dealing here with situations that differ quite radically from the either the artificial intelligence of the Cold War, or the expert systems that followed them in the 1970s and 1980s. Both still assumed that there could be in principle separation or discrmination. But 

1979: Decisions, Ensembles and clusters: the lone expert replaced by statistical algorithm that learns from the data (decision trees - 1980s); the algorithm replaced by ensembles of algorithms (2000) that collectively learn.  Cluster has this double sense of finding groups (the verb to cluster) and a collection (the cluster). Cluster as infrastructural principle (Google Compute 2011)

## The pluri-dimensional space

These are just some examples of techniques flowing through a vast and complex space of scientific knowledges, techniques, technologies, infrastructures, disciplines. They are hundreds of other techniques in these spaces, and literally thousands of implementations and variations.
Techniques flow out of the closed-world spaces of the Cold War labs and research facilities. They become banal devices rather than instruments of a decisionistic elite.
Another thing is taking shape here – a kind of space whose dimensions are practically treated as infinite, and whose potentially infinite expansion animates the massive build out of infrastructures and the intense efforts to scale up and scale down circuitry and electronic devices.  (e.g. NSA)

We need to think about how we inhabit this space. It is a spectral entity in the sense that it comes into physical forms. But some things stand out it. 

1. It is a cursed space. Bellman. That is, the curse of dimensionality means that we cannot inhabit it comfortably. Expanding it to encompass opens up huge voids that have to be managed because they destabilise patterns and boundaries. These feels like ColdWar geopolitics – reorganise the world in order to stabilise forms of power.
2. The expansion of this space sometimes come from inversions of living into non-living, but also from convolutions – the multiplication of things by other things – a series of operations animate its reshaping;
3. It is a space that can be characterised by some geometrical intuitions – lines, planes, surfaces – but constantly eludes that characterisation, and necessitates various attempts to navigate it, to reduce, to compel it to take on visual forms that we can sense. 
4. It is an uneventful space in the sense that nothing happens there. It is like heaven in that sense. But the topology of the space – the way that it is folded, stretched, limited, bounded, open or closed matters in that all manner of decisions, on many different scales, depend on this space and its topology. 

## Conclusions

Hard to get a sense today of the liveness and immediacy of things is rooted in the accumulated sediment of techniques of working with numbers, information or data running back more than a century. 
Important to grasp the emergence of  a problematization associated with data and information that is no reducible to sorting or searching or calculation in any simple sense. There are too many layers of abstraction practice and materiality that would be wiped out by collapsing the decision or the classification down to electronics/logic/code. 
We can't access this problematization and its topological form at the level of use or practice. It is more spectral in its configuration than that. 

## References

Bousquet, A., 2008. Cyberneticizing the American war machine: science and computers in the Cold War. Cold War Hist. 8, 77–102.
Bowker, G., 1993. How to be Universal: Some Cybernetic Strategies, 1943-70. Soc. Stud. Sci. 23, 107–127.
Edwards, P.N., 1996. The closed world : computers and the politics of discourse in Cold War, Inside technology. MIT Press, Cambridge, Mass. ; London.
Solovey, M., Cravens, H., 2012. Cold War Social Science: Knowledge Production, Liberal Democracy, and Human Nature. Palgrave Macmillan.


